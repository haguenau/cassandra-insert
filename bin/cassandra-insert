#! /usr/bin/perl -w
use strict;
require 5.10.0;
use Cassandra::Client;
use Getopt::Long;
use Time::HiRes qw(time sleep);
use JSON;
use FindBin qw($RealBin $Script);
use lib $RealBin . '/../lib';
use BinaryCodec qw(:all);
use File::Temp qw(tempdir);

my $self = $RealBin . '/' . $Script;

## Parse config file
my $fconf = $RealBin . '/../conf/cassandra-insert.json';
open(FCONF, $fconf)
  or die "Couldn't open \`$fconf' for reading: $!";
my $conf;
{
    local $/ = undef;
    $conf = from_json <FCONF>;
}
close FCONF;

my @seed_hosts = @{$conf->{nodes} // []};
my $db = $conf->{db};

my %metrics;
my $time_start = time;
my $duration_slept = 0;

## Parse command line (overrides conf)
my $table;
my $ttl = 86400;

my $concurrent_async_inserts = 32;
my $throttle_interval = 1000;   # Adjust for pacing after this many inserts
my $throttle_wps = 10000;       # Set writes per second limit

my $def_constants;
my $def_types;
my $is_verbose = 0;
my $jobs = 16;
my $connections = 64;
my ($do_print_json, $do_print_kv) = (0, 0);
my ($write_json, $write_kv);
exit 1 unless GetOptions
  (
   'connections' => \$connections, # max_connections, per worker
   'const=s' => \$def_constants,
   'db=s' => \$db,
   'header=s' => \$def_types,
   'j|jobs=i' => \$jobs,
   'nodes=s' => sub { @seed_hosts = split /,/, $_[1]; },
   'print-json' => \$do_print_json,
   'print-kv' => \$do_print_kv,
   'table=s' => \$table,
   't|throttle-wps=i' => \$throttle_wps,
   'ttl=i' => \$ttl,
   'v|verbose' => \$is_verbose,
   'write-json=s' => \$write_json,
   'write-kv=s' => \$write_kv,
  );

sub identity($) { return shift; }
my %converters =
  (
   'auto' => \&identity,
   'delta-varint-list' => \&BinaryCodec::delta_varint_list_encode,
  );

my @keys = qw(key column1 column2 value);
my %keys = map { $_ => 1 } @keys;
my %convert = ();

my @callbacks = ();

sub check_params() {
    my %mustset =
      (
       const => \$def_constants,
       db => \$db,
       header => \$def_types,
       table => \$table,
      );

    my $ok = 1;
    for my $k (keys %mustset) {
        unless (defined ${$mustset{$k}}) {
            warn "Error: parameter \`$k' is unset.\n";
            $ok = 0;
        }
    }
    exit 1 unless $ok;
}

sub check_get_converter($) {
    my $type = shift;
    my $converter = $converters{$type};

    die "No converter for type \`$type'" unless defined $converter;
    return $converter;
}

sub check_key_definable($) {
    my $key = shift;
    die "Bad key \`$key'" unless defined $keys{$key};
}

sub parse_constants() {
    ## fmt: <id>:<type>=<value>{,<id>:<type>=<value>}
    my @conststrs = split /,/, $def_constants;
    for my $constdef (@conststrs) {
        my ($k, $t, $v) = $constdef =~ /^([a-z0-9-]+):([a-z0-9-]+)=(.*)$/;
        die "Bad constant definition string \`$constdef'" unless defined $v;
        die "Duplicate constant key \`$k'" if defined $convert{$k};
        die "Invalid constant key \`$k'" unless defined $keys{$k};

        my $converter = check_get_converter $t;
        $convert{$k} = { value => $converter->($v) };
    }
}

sub parse_header() {
    ## fmt: <id>:<type>{,<id>:<type>}
    my $column_count;
    {
        my @headstrs = split /,/, $def_types;
        $column_count = @headstrs;
        my $i = 0;
        for my $typedef (@headstrs) {
            my ($k, $t) = $typedef =~ /^([a-z0-9-]+):([a-z0-9-]+)$/;
            die "Bad type declaration string \`$typedef'" unless defined $t;
            die "Duplicate data key \`$k'" if defined $convert{$k};
            die "Invalid data key \`$k'" unless defined $keys{$k};

            $convert{$k} = { convert => check_get_converter $t, index => $i++ };
        }
    }

    return $column_count;
}

sub check_keys() {
    my @missing_keys = ();
    for my $k (sort keys %keys) {
        push @missing_keys, $k unless defined $convert{$k};
    }

    die sprintf "Must set all keys; missing %s",
      join ', ', map {  "\`$_'" } @missing_keys if @missing_keys;
}

sub throttle_sleep($$) {
    my ($wps, $writes_completed) = @_;
    return unless defined $wps or $wps <= 0;

    my $target_duration = $writes_completed / $wps;
    my $target_time = $time_start + $target_duration;
    my $now = time;

    my $actual_wps = $writes_completed / ($now - $time_start);
    if ($now < $target_time) {
        my $sleep_seconds = $target_time - $now;
        printf STDERR "Throttle: sleep %.2f (writes: %.1f/s avg).\n",
          $sleep_seconds, $actual_wps
          if $is_verbose;
        sleep $sleep_seconds;
        $duration_slept += $sleep_seconds;
    } elsif ($is_verbose) {
        printf STDERR "Writes: %.1f/s avg.\n", $actual_wps;
    }
}

sub run_inserts($) {
    my $column_count = shift;
    my $cass = Cassandra::Client->new
      (
       contact_points => \@seed_hosts,
       keyspace => $db,
       max_connections => $connections,
      );

    $cass->connect;

    my $query = sprintf
      'INSERT INTO "%s" (%s) VALUES (?, ?, ?, ?)',
      $table,
      join ', ', @keys;
    my $ttl_query = ' USING TTL ?';

    $query .= $ttl_query if defined $ttl;

    my ($recs_in, $recs_out, $bad_recs, $write_failures) = (0, 0, 0, 0);
    my $update_counts = sub {
        if (shift) {
            ++$recs_out;
        } else {
            ++$write_failures;
            printf STDERR "FAIL: %s\n", join ', ', $@;
        }
    };

    while (<STDIN>) {
        ++$recs_in;
        chomp;
        my @data = split /\t/;
        if (@data != $column_count) {
            ++$bad_recs;
            next;
        }

        my @values = ();
        for my $k (@keys) {
            my $entry = $convert{$k};
            my $value;

            if (defined $entry->{value}) {
                $value = $entry->{value};
            } else {
                my $converter = $entry->{convert};
                my $str = $data[$entry->{index}];
                $value = $converter->($str);
            }
            push @values, $value;
        }
        push @values, $ttl if defined $ttl;

        push @callbacks, $cass->future_execute($query, \@values);

        if (@callbacks >= $concurrent_async_inserts) {
            my $ok = eval { (shift @callbacks)->() };
            &$update_counts($ok);
            if (($recs_out % $throttle_interval) == 0) {
                throttle_sleep $throttle_wps, $recs_out;
            }
        }
    }

    ## Flush any remaining queries at end of input
    for my $cb (@callbacks) {
        my $ok = eval { $cb->() };
        &$update_counts($ok);
    }

    sub round($) { 0 + sprintf "%.2f", shift; }
    %metrics =
      (
       recs_in => $recs_in,
       recs_out => $recs_out,
       bad_recs => $bad_recs,
       write_failures => $write_failures,
       seconds_slept => round $duration_slept,
       seconds_total => round time - $time_start,
       unix_time => int $time_start,
      );
}

sub as_json($) {
    my $data = shift;
    return to_json($data, { canonical => 1}) . "\n";
}

## Pretty-print in a Nagios-friendly <key> <space> <value> format
sub as_kv($) {
    my $data = shift;
    my $out = '';
    for my $k (keys %$data) {
        $out .= sprintf "%s %s\n", $k, $data->{$k};
    }

    return $out;
}

sub print_metrics($) {
    my $metrics = shift;
    my $json = as_json $metrics;
    my $kv = as_kv $metrics;

    print $json if $do_print_json;
    print $kv if $do_print_kv;

    for my $conf ({ file => $write_json, data => $json },
                  { file => $write_kv, data => $kv }) {
        if (defined $conf->{file}) {
            my $fout = $conf->{file};
            my $ftmp = $fout . '.tmp';
            open(FOUT, ">$ftmp")
              or die "Couldn't open \`$ftmp' for writing: $!";
            print FOUT $conf->{data};
            close FOUT;

            printf STDERR "Rename to \`$fout' failed: $!\n"
              unless rename $ftmp, $fout;
        }
    }
}

sub main() {
    check_params;
    parse_constants;
    my $column_count = parse_header;
    check_keys;

    run_inserts $column_count;
    print_metrics \%metrics;
}

sub dispatch() {
    ## Build command line to be passed down to workers
    my @args =
      (
       '--const', $def_constants,
       '--db', $db,
       '--header', $def_types,
       '--jobs', 0,
       '--nodes', join (',', @seed_hosts),
       '--table', $table,
       '--throttle-wps', int $throttle_wps / $jobs,
       '--ttl', $ttl,
      );
    push @args, '--verbose' if $is_verbose;

    my $tmpdir = tempdir("$Script.XXXXXXXX", TMPDIR => 1, CLEANUP => 1);
    my @fhs = ();
    my @tmps = ();
    for my $i (1 .. $jobs) {
        my $fh;
        my $jsonout = sprintf '%s/worker-%04d.json', $tmpdir, $i;
        push @tmps, $jsonout;
        open ($fh, '|-', $self, @args, '--write-json', $jsonout)
          or die $!;
        push @fhs, $fh;
    }

    ## Copy STDIN lines to children, in sequence
    my $i = 0;
    while (my $l = <STDIN>) {
        my $fh = $fhs[$i++ % @fhs];
        print $fh $l;
    }

    ## Close & collect
    for my $fh (@fhs) {
        close $fh;
    }

    ## Aggregate metrics from each worker
    my %metrics = ();
    my @keys_sum = qw(bad_recs recs_in recs_out seconds_slept
                      seconds_total write_failures);
    my @keys_max = qw(unix_time);
    for my $f (@tmps) {
        open (FIN, $f) or next;
        local $/ = undef;
        my $json = <FIN>;
        close FIN;

        my $this = from_json $json;
        for my $k (@keys_sum) { $metrics{$k} += $this->{$k} }
        for my $k (@keys_max) {
            $metrics{$k} //= $this->{$k};
            $metrics{$k} = $this->{$k} if $metrics{$k} < $this->{$k};
        }
    }

    print_metrics \%metrics;
}

if ($jobs > 1) {
    ## Master: start children and give them work
    dispatch;
} else {
    main;
}
